{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f602e8ac-8742-48ee-a7e9-c18fa24345cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f874219e-6723-4fcd-989d-d28fb603a270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1391e451-0c90-4fc9-bfd1-d6d471009fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=5000),\n",
    "    \"RandomForest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeea20c0-f322-4c84-a386-1424e6aa99ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"solver\": [\"liblinear\", \"lbfgs\"]\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [None, 5, 10]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"kernel\": [\"linear\", \"rbf\"]\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"n_neighbors\": [3, 5, 7, 9],\n",
    "        \"weights\": [\"uniform\", \"distance\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebef67f7-f5de-4b4a-9684-983114b73dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hardika\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hardika\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LogisticRegression ---\n",
      "Best Params: {'C': 0.1, 'solver': 'liblinear'}\n",
      "Accuracy: 0.9942\n",
      "Precision: 0.9908\n",
      "Recall: 1.0000\n",
      "F1-score: 0.9954\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        63\n",
      "           1       0.99      1.00      1.00       108\n",
      "\n",
      "    accuracy                           0.99       171\n",
      "   macro avg       1.00      0.99      0.99       171\n",
      "weighted avg       0.99      0.99      0.99       171\n",
      "\n",
      "-----\n",
      "\n",
      "--- RandomForest ---\n",
      "Best Params: {'max_depth': 10, 'n_estimators': 100}\n",
      "Accuracy: 0.9708\n",
      "Precision: 0.9640\n",
      "Recall: 0.9907\n",
      "F1-score: 0.9772\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96        63\n",
      "           1       0.96      0.99      0.98       108\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.97      0.96      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n",
      "-----\n",
      "\n",
      "--- SVM ---\n",
      "Best Params: {'kernel': 'linear', 'C': 0.1}\n",
      "Accuracy: 0.9825\n",
      "Precision: 0.9817\n",
      "Recall: 0.9907\n",
      "F1-score: 0.9862\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98        63\n",
      "           1       0.98      0.99      0.99       108\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.98      0.98       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n",
      "-----\n",
      "\n",
      "--- KNN ---\n",
      "Best Params: {'weights': 'uniform', 'n_neighbors': 3}\n",
      "Accuracy: 0.9591\n",
      "Precision: 0.9633\n",
      "Recall: 0.9722\n",
      "F1-score: 0.9677\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94        63\n",
      "           1       0.96      0.97      0.97       108\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.96      0.95      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "searches = {}\n",
    "\n",
    "# GridSearchCV for LogisticRegression and RandomForest\n",
    "for name in [\"LogisticRegression\", \"RandomForest\"]:\n",
    "    clf = models[name]\n",
    "    grid = GridSearchCV(clf, param_grids[name], cv=5, scoring='accuracy')\n",
    "    grid.fit(X_train, y_train)\n",
    "    searches[name] = grid\n",
    "\n",
    "# RandomizedSearchCV for SVM and KNN\n",
    "svc = models[\"SVM\"]\n",
    "svc_search = RandomizedSearchCV(\n",
    "    svc, param_grids[\"SVM\"], cv=5, n_iter=10, scoring='accuracy', random_state=42\n",
    ")\n",
    "svc_search.fit(X_train, y_train)\n",
    "searches[\"SVM\"] = svc_search\n",
    "\n",
    "knn = models[\"KNN\"]\n",
    "knn_search = RandomizedSearchCV(\n",
    "    knn, param_grids[\"KNN\"], cv=5, n_iter=10, scoring='accuracy', random_state=42\n",
    ")\n",
    "knn_search.fit(X_train, y_train)\n",
    "searches[\"KNN\"] = knn_search\n",
    "\n",
    "# Evaluate and compare models\n",
    "for name, search in searches.items():\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"Best Params:\", search.best_params_)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"-\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3511f318-71e1-48ba-b760-b6337fa1239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Model Comparison (After Tuning):\n",
      "\n",
      "                Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  LogisticRegression  0.994152   0.990826  1.000000  0.995392\n",
      "2                 SVM  0.982456   0.981651  0.990741  0.986175\n",
      "1        RandomForest  0.970760   0.963964  0.990741  0.977169\n",
      "3                 KNN  0.959064   0.963303  0.972222  0.967742\n",
      "\n",
      " Best Performing Model: LogisticRegression with F1 Score: 0.9954\n"
     ]
    }
   ],
   "source": [
    "#  final results into a list of dicts\n",
    "results = []\n",
    "\n",
    "for name, search in searches.items():\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "final_results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by F1 Score\n",
    "final_sorted = final_results_df.sort_values(by='F1 Score', ascending=False)\n",
    "\n",
    "print(\"\\n Final Model Comparison (After Tuning):\\n\")\n",
    "print(final_sorted)\n",
    "\n",
    "# Best Model Summary\n",
    "best_model = final_sorted.iloc[0]\n",
    "print(f\"\\n Best Performing Model: {best_model['Model']} with F1 Score: {best_model['F1 Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913150b-0cb7-4cc9-afbe-db1632764536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
